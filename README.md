# Awesome-Multimodel-learning
# ğŸŒ Awesome Multimodal Learning & Foundation Models

ğŸ“š A curated collection of resources related to **Multimodal Learning** and **Multimodal Foundation Models**, including applications in healthcare, vision-language modeling, tutorials, and open-source tools.

---

## ğŸ“Œ Table of Contents

- [ğŸ§  Tutorials & Workshops](#-tutorials--workshops)
- [ğŸ› Research Labs](#-research-labs)
- [ğŸ“Š Applications in Healthcare](#-applications-in-healthcare)
- [ğŸ§° Code & Tools](#-code--tools)
- [ğŸ“š Suggested Papers](#-suggested-papers)
- [ğŸ¤ Contributing](#-contributing)

---

## ğŸ§  Tutorials & Workshops

- ğŸ“ [CVPR 2024 Multimodal LLM Workshop](https://mllm2024.github.io/CVPR2024/)
- ğŸ“ [ICML 2023 Tutorial on Multimodal Machine Learning](https://cmu-multicomp-lab.github.io/mmml-tutorial/icml2023/)

---

## ğŸ› Research Labs

- ğŸ« [CMU Multicomp Lab (Homepage)](http://multicomp.cs.cmu.edu/)
- ğŸ”¬ [CMU Multicomp Research Projects](http://multicomp.cs.cmu.edu/research/)
- ğŸ§© [CMU Multimodal Machine Learning Portal](http://multicomp.cs.cmu.edu/multimodal-machine-learning/)
- ğŸ’» [CMU Multicomp GitHub](https://github.com/CMU-MultiComp-Lab)

---

## ğŸ“Š Applications in Healthcare

- ğŸ§  [Google Research â€“ Multimodal Medical AI](https://research.google/blog/multimodal-medical-ai/)
- ğŸ©º [Owkin: Aâ€“Z of AI in Healthcare](https://www.owkin.com/a-z-of-ai-in-healthcare)
- ğŸ”¬ [Owkin â€“ Multimodal Data in Healthcare](https://www.owkin.com/a-z-of-ai-for-healthcare/multimodal-data)
- ğŸ” [HIT Webinar è°¢é›¨å½¤ MUZUAI](https://meeting.tencent.com/crm/2qP88r5Rc6)

---

## ğŸ§° Code & Tools

- ğŸ”— [CMU MultiComp GitHub](https://github.com/CMU-MultiComp-Lab)
- ğŸ¤– [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)
- ğŸ§  [LLaVA (Large Language and Vision Assistant)](https://github.com/haotian-liu/LLaVA)
- ğŸ“¦ [BLIP / BLIP-2 (Salesforce)](https://github.com/salesforce/BLIP)
- ğŸ›  [HuggingFace Transformers â€“ Multimodal Support](https://huggingface.co/docs/transformers/index)

---

## ğŸ“š Suggested Papers

| Title | Year | Link |
|-------|------|------|
| Multimodal Transformers: A Survey | 2022 | [arXiv](https://arxiv.org/abs/2206.06488) |
| A Survey on Multimodal Learning | 2023 | [arXiv](https://arxiv.org/abs/2304.05499) |
| A Survey on Multimodal Foundation Models | 2023 | [arXiv](https://arxiv.org/abs/2307.12339) |
| Vision-Language Pre-training: A Survey | 2022 | [arXiv](https://arxiv.org/abs/2112.04426) |
| Self-Supervised Multimodal Learning: A Survey | 2021 | [arXiv](https://arxiv.org/abs/2107.08691) |

---

## ğŸ¤ Contributing

Contributions are welcome! If you know of other great multimodal resources, please feel free to open a pull request.

---

## ğŸ“„ License

This repository is licensed under the MIT License.
